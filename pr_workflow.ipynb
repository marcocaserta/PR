{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb4ff95-c131-4ea3-a76d-0ae41e48d8a5",
   "metadata": {},
   "source": [
    "# PR Processing Workflow\n",
    "\n",
    "Dec. 16, 2023: Refactoring the code of `read_files.ipynb`, to define the pipeline. The code below covers 4 steps (plus one extra step, not used in the final analysis) and assumes that the excel files containing the PR text have already been produced. To extract the text from the html and pdf files, see `read_files.ipynb`. We thus assume that the folder `data` contains one file for each company, with name `{company}.xlsx`, with a column `text` containing the processed text of each PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e72507b-7db3-4fe8-8a03-123e99d6f37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/desktop-notify+json": {
       "id": "00606bca-03ef-4e72-aeeb-13ed017701dc",
       "isProcessed": true,
       "payload": {
        "title": null
       },
       "type": "INIT"
      },
      "text/plain": [
       "<jupyterlab_notify.magics._Notification at 0x2a9525a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%load_ext lab_black\n",
    "%load_ext jupyterlab_notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7578c30f-dcf6-4410-a31b-1ba654b10eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 30 companies.\n"
     ]
    }
   ],
   "source": [
    "companies_all = [\n",
    "    \"Acerinox\",\n",
    "    \"ACS\",\n",
    "    \"Bancosantander\",\n",
    "    \"Bankinter\",\n",
    "    \"BBVA\",\n",
    "    \"Caixa\",\n",
    "    \"Colonial\",\n",
    "    \"Enagas\",\n",
    "    \"Endesa\",\n",
    "    \"Ferrovial\",\n",
    "    \"Grifols\",\n",
    "    \"IAG\",\n",
    "    \"Iberdrola\",\n",
    "    \"Inditex\",\n",
    "    \"Acciona\",\n",
    "    \"Arcelormittal\",\n",
    "    \"Bancosabadell\",\n",
    "    \"Cellnex\",\n",
    "    \"Fluidra\",\n",
    "    \"Indra\",\n",
    "    \"Logista\",\n",
    "    \"Melia\",\n",
    "    \"Merlin\",\n",
    "    \"Naturgy\",\n",
    "    \"Red\",\n",
    "    \"Repsol\",\n",
    "    \"Rovi\",\n",
    "    \"Sacyr\",\n",
    "    \"Solaria\",\n",
    "    \"Telefonica\",\n",
    "]\n",
    "print(f\"Analyzing {len(companies_all)} companies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d0c0f-edbb-4c04-829b-0be9e5425250",
   "metadata": {},
   "source": [
    "## Code and Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d5b4a4-8778-4844-8340-1e7069f14479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(docs_list, stopwords):\n",
    "    sentences = []\n",
    "    for doc in docs_list:\n",
    "        aux = sent_tokenize(doc)\n",
    "        for s in aux:\n",
    "            # print(\"s = \", s)\n",
    "            tokens = nltk.tokenize.wordpunct_tokenize(s)\n",
    "            s = \" \".join([w for w in tokens if w.isalnum() and w not in stopwords])\n",
    "            sentences.append(s)\n",
    "            # print(sentences)\n",
    "            # input(\"aka\")\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_embeddings(docs):\n",
    "    sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_topics(language, sentences, embeddings, nr_topics=10):\n",
    "\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=\"english\"\n",
    "    )  # should be ignored in spanish (stop_words='spanish' does not exist)\n",
    "    ctfidf_model = ClassTfidfTransformer(\n",
    "        reduce_frequent_words=True, bm25_weighting=True\n",
    "    )\n",
    "    # representation_model = KeyBERTInspired()\n",
    "    representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "    # topic_model = BERTopic(embedding_model= 'paraphrase-multilingual-MiniLM-L12-v2', representation_model=representation_model,\n",
    "    #                       vectorizer_model=vectorizer_model, ctfidf_model=ctfidf_model, nr_topics=nr_topics, reduce_frequent_words=True, verbose=True)\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        representation_model=representation_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        nr_topics=nr_topics,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # topic_model = BERTopic(embedding_model= 'LaBSE', representation_model=representation_model, vectorizer_model=vectorizer_model, ctfidf_model=ctfidf_model, nr_topics=nr_topics)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(sentences, embeddings)\n",
    "\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "\n",
    "def topics_modeling(company):\n",
    "\n",
    "    for label, language, bert_language in zip(labels, languages, bert_languages):\n",
    "        print(f\"\\t - Language : {language}\")\n",
    "        filename = os.path.join(\n",
    "            \"data\", \"sentences\", (\"sentences_\" + company + \"_\" + label + \".pkl\")\n",
    "        )\n",
    "        sentences = pickle.load(open(filename, \"rb\"))\n",
    "        print(\"[S1.] \\t Sentences Imported.\")\n",
    "\n",
    "        filename = os.path.join(\"data\", \"embeddings\", (company + \"_\" + label + \".pkl\"))\n",
    "        embs = pickle.load(open(filename, \"rb\"))\n",
    "        print(\n",
    "            f\"[S2.] \\t Embeddings Imported. Starting topic modeling using transformer 'paraphrase-multilingual-MiniLM-L12-v2'...\"\n",
    "        )\n",
    "        model, topic, prob = get_topics(\n",
    "            language=language, sentences=sentences, embeddings=embs\n",
    "        )\n",
    "        print(\"[S2.] \\t Topics Modeling done.\")\n",
    "        print(model.get_topic_info())\n",
    "        filename = os.path.join(\"models\", (company + \"_\" + label + \".bert\"))\n",
    "        model.save(filename)\n",
    "        print(f\"[S3.] \\t Model {filename} saved to disk.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def sentence_tokenize(company, df):\n",
    "\n",
    "    print(\"\\n[S1.] Tonekize sentences\")\n",
    "    for label, language, bert_language in zip(labels, languages, bert_languages):\n",
    "        print(f\"\\t - Language : {language}\")\n",
    "        doc_list = df[df.language == label].text.values\n",
    "        stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        sentences = get_sentences(doc_list, stopwords)\n",
    "        filename = os.path.join(\n",
    "            \"data\", \"sentences\", (\"sentences_\" + company + \"_\" + label + \".pkl\")\n",
    "        )\n",
    "        pickle.dump(sentences, open(filename, \"wb\"))\n",
    "        print(\n",
    "            f\"\\t[S1-{label}.] \\t Sentences tonenized saved to disk. Total nr. sentences = {len(sentences)}.\"\n",
    "        )\n",
    "    print(\"[S1.] Done with tonekize sentences\")\n",
    "\n",
    "\n",
    "def embeddings_creation(company):\n",
    "    print(\"\\n[S2.] Embeddings creation\")\n",
    "    for label, language, bert_language in zip(labels, languages, bert_languages):\n",
    "        filename = os.path.join(\n",
    "            \"data\", \"sentences\", (\"sentences_\" + company + \"_\" + label + \".pkl\")\n",
    "        )\n",
    "        sentences = pickle.load(open(filename, \"rb\"))\n",
    "        print(f\"[S2-{label}.] \\t Starting with embeddings creation.\")\n",
    "        embs = get_embeddings(sentences)\n",
    "        filename = os.path.join(\"data\", \"embeddings\", (company + \"_\" + label + \".pkl\"))\n",
    "        print(f\"[S2-{label}.] \\t Embeddings file {filename} saved to disk.\")\n",
    "        pickle.dump(embs, open(filename, \"wb\"))\n",
    "    print(\"[S2.] Done with embeddings creation\")\n",
    "\n",
    "\n",
    "def pr_2_pr(companies, transformer_type=\"laBSE\", en_to_es=True, save_file=False):\n",
    "    # sentence transformer to find 1:1 match\n",
    "    # use spanish as corpus and english for queries\n",
    "\n",
    "    print(\n",
    "        f\"\\t [NOTE] Similarity scores computed using '{transformer_type}' transformer\"\n",
    "    )\n",
    "    model = SentenceTransformer(transformer_type)\n",
    "\n",
    "    similarity_scores = []\n",
    "    nr_ens = []\n",
    "    nr_ess = []\n",
    "\n",
    "    print(f\"** Sentence Transformers (match) for Company '{company}' **\")\n",
    "    name = company + \".xlsx\"\n",
    "    df = pd.read_excel(os.path.join(\"data\", name))\n",
    "    df = df[~df.text.isna()]\n",
    "    nr_en = df[df.language == \"en\"].shape[0]\n",
    "    nr_es = df[df.language == \"es\"].shape[0]\n",
    "    en_list = df[df.language == \"en\"].text.to_list()\n",
    "    en_files = df[df.language == \"en\"].filename.to_list()\n",
    "    es_list = df[df.language == \"es\"].text.to_list()\n",
    "    es_files = df[df.language == \"es\"].filename.to_list()\n",
    "    # stopwords = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "\n",
    "    if en_to_es:\n",
    "        corpus_embedding = model.encode(es_list, convert_to_tensor=True)\n",
    "        top_k = min(1, len(es_list))\n",
    "        corpus_list = es_list\n",
    "        corpus_files_list = es_files\n",
    "        query_list = en_list\n",
    "        query_files_list = en_files\n",
    "        name = \"comparison_\" + company + \"_en2es.xlsx\"\n",
    "    else:\n",
    "        corpus_embedding = model.encode(en_list, convert_to_tensor=True)\n",
    "        top_k = min(1, len(en_list))\n",
    "        corpus_list = en_list\n",
    "        corpus_files_list = en_files\n",
    "        query_list = es_list\n",
    "        query_files_list = es_files\n",
    "        name = \"comparison_\" + company + \"_es2en.xlsx\"\n",
    "\n",
    "    best_match = []\n",
    "    best_score = []\n",
    "    best_name = []\n",
    "    for query in tqdm(query_list):\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embedding)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "        # print(\"Query:\", query)\n",
    "        # print(\"---------------------------\")\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            # print(f'[{idx:4d}]\\t{round(score.item(), 3)} | {corpus_list[idx]}')\n",
    "            best_match.append(idx)\n",
    "            best_score.append(score.item())\n",
    "            best_name.append(corpus_files_list[idx])\n",
    "    if en_to_es:\n",
    "        print(\n",
    "            f\"{company:15s}\\t similarity score from 'en' to 'es' = {np.mean(best_score):.3f}.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"{company:15s}\\t similarity score from 'es' to 'en' = {np.mean(best_score):.3f}.\"\n",
    "        )\n",
    "\n",
    "    similarity_scores.append(np.mean(best_score))\n",
    "    nr_ens.append(nr_en)\n",
    "    nr_ess.append(nr_es)\n",
    "    if save_file:\n",
    "        df_comparison = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"similarity\": best_score,\n",
    "                    \"query\": query_list,\n",
    "                    \"match\": [corpus_list[i] for i in best_match],\n",
    "                    \"idx_match\": [int(i) for i in best_match],\n",
    "                    \"query_filename\": query_files_list,\n",
    "                    \"match_filename\": best_name,\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"idx_query\"})\n",
    "        )\n",
    "        filename = os.path.join(\"results/best_matching/\", name)\n",
    "        df_comparison.to_excel(filename, index=False)\n",
    "        print(\"Comparison file saved to disk : '{}'.\".format(filename))\n",
    "    return similarity_scores, nr_ens, nr_ess\n",
    "\n",
    "\n",
    "def get_similarity_scores(company, transformer_type, with_printing=True):\n",
    "    \"\"\"Get pr_2_pr similarity score, using a query and finding the best match.\n",
    "\n",
    "    NOTE: For Rovi, Santander, and Solaria, manually change header of\n",
    "    column B in the data/company.xlslx excel file (to \"filename\")\"\"\"\n",
    "\n",
    "    from_en_to_es = [True, False]\n",
    "    suffixes = [\"en2es\", \"es_to_en\"]\n",
    "\n",
    "    for en_to_es, suffix in zip(from_en_to_es, suffixes):\n",
    "        similarity_scores, nr_ens, nr_ess = pr_2_pr(\n",
    "            company, transformer_type, en_to_es=en_to_es, save_file=True\n",
    "        )\n",
    "\n",
    "        df_sim = pd.DataFrame(\n",
    "            {\n",
    "                \"company\": company,\n",
    "                \"similarity\": similarity_scores,\n",
    "                \"nr_en\": nr_ens,\n",
    "                \"nr_es\": nr_ess,\n",
    "            }\n",
    "        )\n",
    "        if with_printing:\n",
    "            print(tabulate(df_sim, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "def get_summary_similarity(companies):\n",
    "    nr_ess = np.empty(len(companies))\n",
    "    nr_ens = np.empty(len(companies))\n",
    "    sim_en2es = np.empty(len(companies))\n",
    "    sim_es2en = np.empty(len(companies))\n",
    "\n",
    "    for i, company in enumerate(tqdm(companies)):\n",
    "        df_en_es = pd.read_excel(\n",
    "            f\"results/best_matching/comparison_{company}_en2es.xlsx\"\n",
    "        )\n",
    "        df_es_en = pd.read_excel(\n",
    "            f\"results/best_matching/comparison_{company}_es2en.xlsx\"\n",
    "        )\n",
    "        nr_ess[i] = df_es_en.shape[0]\n",
    "        nr_ens[i] = df_en_es.shape[0]\n",
    "        sim_en2es[i] = df_en_es.similarity.mean()\n",
    "        sim_es2en[i] = df_es_en.similarity.mean()\n",
    "\n",
    "    df_temp = pd.DataFrame(\n",
    "        {\n",
    "            \"company\": companies,\n",
    "            \"nr_en\": nr_ens,\n",
    "            \"nr_es\": nr_ess,\n",
    "            \"similarity_en_to_es\": sim_en2es,\n",
    "            \"similarity_es_to_en\": sim_es2en,\n",
    "        }\n",
    "    ).sort_values(by=\"company\")\n",
    "    filename = \"results/avg_similarity_company_both.xlsx\"\n",
    "    df_temp.to_excel(filename, index=False)\n",
    "    print(f\"Saving summary table to disk file: '{filename}'.\")\n",
    "    print(tabulate(df_temp, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "def match_topics(company):\n",
    "    \"\"\"Try to find the best match between topics of the same company in English and Spanish.\"\"\"\n",
    "\n",
    "    for company in companies:\n",
    "        model_en = BERTopic.load(f\"models/{company}_en.bert\")\n",
    "        model_es = BERTopic.load(f\"models/{company}_es.bert\")\n",
    "        M = cosine_similarity(\n",
    "            model_en.topic_embeddings_[1:], model_es.topic_embeddings_[1:]\n",
    "        )\n",
    "        idx = np.argmax(M, axis=1).tolist()  # best match for each english topic\n",
    "        dfE = pd.DataFrame(model_en.get_topic_info()[1:]).rename(\n",
    "            columns={\"Name\": \"Name EN\"}\n",
    "        )\n",
    "        dfS = (\n",
    "            pd.DataFrame(model_es.get_topic_info().loc[[i + 1 for i in idx]])\n",
    "            .reset_index()\n",
    "            .rename(columns={\"Name\": \"Best Match ES\"})\n",
    "        )\n",
    "        dfMatch = (\n",
    "            pd.concat(\n",
    "                [dfE.reset_index(), dfS.reset_index(), pd.DataFrame(M.max(axis=1))],\n",
    "                axis=1,\n",
    "            )\n",
    "            .rename(columns={0: \"similarity\"})\n",
    "            .drop(columns={\"index\", \"level_0\"})\n",
    "        )\n",
    "        print(\n",
    "            f\"[{company:>20s}]\\tTotal similarity score = {dfMatch.similarity.mean():.3f}\"\n",
    "        )\n",
    "        dfMatch.to_excel(f\"results/topics/topics_match_{company}.xlsx\", index=False)\n",
    "        print(f\"Saving file 'results/topics/topics_match_{company}.xlsx' to disk.\")\n",
    "\n",
    "        cols = [\"topic_\" + str(i) for i in range(dfMatch.shape[0])]\n",
    "        df_en = pd.DataFrame(model_en.get_topics()).iloc[:, 1:]\n",
    "        df_es = pd.DataFrame(model_es.get_topics())[idx]\n",
    "        df_en.columns = cols\n",
    "        df_es.columns = cols\n",
    "        pd.concat([df_en, df_es], axis=0).reset_index().to_excel(\n",
    "            f\"results/topics/topics_details_{company}.xlsx\", index=False\n",
    "        )\n",
    "        print(f\"Saving file 'results/topics/topics_details_{company}.xlsx' to disk.\")\n",
    "\n",
    "\n",
    "def topics_on_low_similarity_pr(company, with_printing=True):\n",
    "    \"\"\"Extract topics only using PR with low similarity.\n",
    "\n",
    "    A PR has a low similarity if the similarity score is in the botton 20th percentile.\n",
    "    \"\"\"\n",
    "\n",
    "    suffixes = [\"en2es\", \"es2en\"]\n",
    "    print(f\"** Extracting topics on low similarity PR for Company '{company}' **\")\n",
    "\n",
    "    for label, language, bert_language, suffix in zip(\n",
    "        labels, languages, bert_languages, suffixes\n",
    "    ):\n",
    "        print(f\"\\t - Language : {language}\")\n",
    "        df = pd.read_excel(f\"results/best_matching/comparison_{company}_{suffix}.xlsx\")\n",
    "        doc_list = df[df.similarity < np.percentile(df.similarity, q=20)][\n",
    "            \"query\"\n",
    "        ].values\n",
    "        print(f\"\\tExtracting {len(doc_list)} PR with low similarity...\")\n",
    "\n",
    "        stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        print(\"[S1-s.] \\t Tonekize sentences\")\n",
    "        sentences = get_sentences(doc_list, stopwords)\n",
    "        filename = os.path.join(\n",
    "            \"data\", \"sentences\", (\"sentences_lowsim_\" + company + \"_\" + label + \".pkl\")\n",
    "        )\n",
    "        pickle.dump(sentences, open(filename, \"wb\"))\n",
    "        print(\n",
    "            \"[S1-e.] \\t Sentences tonenized saved to disk. Total nr. sentences = {}.\".format(\n",
    "                len(sentences)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"[S2-s.] \\t Starting with embeddings creation.\")\n",
    "        embs = get_embeddings(sentences)\n",
    "        filename = os.path.join(\n",
    "            \"data\", \"embeddings\", (company + \"_lowsim_\" + label + \".pkl\")\n",
    "        )\n",
    "        print(f\"[S2-e.] \\t Embeddings file {filename} saved to disk.\")\n",
    "        pickle.dump(embs, open(filename, \"wb\"))\n",
    "\n",
    "        embs = pickle.load(open(filename, \"rb\"))\n",
    "        print(\n",
    "            f\"[S2.] \\t Embeddings imported from {filename}. Starting topic modeling...\"\n",
    "        )\n",
    "        model, topic, prob = get_topics(\n",
    "            language=language, sentences=sentences, embeddings=embs\n",
    "        )\n",
    "        print(\"[S2.] \\t Topics Modeled\")\n",
    "        print(model.get_topic_info())\n",
    "        df_topic = pd.DataFrame(model.get_topics()).iloc[:, 1:]\n",
    "        namefile = f\"results/topics/topics_details_lowsim_{company}_{label}.xlsx\"\n",
    "        df_topic.to_excel(namefile, index=False)\n",
    "        print(f\"Topics details for company {company} saved to disk. File '{namefile}'\")\n",
    "        if with_printing:\n",
    "            print(df_topic)\n",
    "\n",
    "\n",
    "def print_summary_similarity(companies):\n",
    "\n",
    "    nr_ess = np.empty(len(companies))\n",
    "    nr_ens = np.empty(len(companies))\n",
    "    sim_en2es = np.empty(len(companies))\n",
    "    sim_es2en = np.empty(len(companies))\n",
    "\n",
    "    for i, company in enumerate(tqdm(companies)):\n",
    "        df_en_es = pd.read_excel(\n",
    "            f\"results/best_matching/comparison_{company}_en2es.xlsx\"\n",
    "        )\n",
    "        df_es_en = pd.read_excel(\n",
    "            f\"results/best_matching/comparison_{company}_es2en.xlsx\"\n",
    "        )\n",
    "        nr_ess[i] = df_es_en.shape[0]\n",
    "        nr_ens[i] = df_en_es.shape[0]\n",
    "        sim_en2es[i] = df_en_es.similarity.mean()\n",
    "        sim_es2en[i] = df_es_en.similarity.mean()\n",
    "\n",
    "    df_temp = pd.DataFrame(\n",
    "        {\n",
    "            \"company\": companies,\n",
    "            \"nr_en\": nr_ens,\n",
    "            \"nr_es\": nr_ess,\n",
    "            \"similarity_en_to_es\": sim_en2es,\n",
    "            \"similarity_es_to_en\": sim_es2en,\n",
    "        }\n",
    "    ).sort_values(by=\"company\")\n",
    "    df_temp.to_excel(\"results/avg_similarity_company_both.xlsx\", index=False)\n",
    "    print(tabulate(df_temp, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933a2fb-a4d2-42aa-ba4f-36d5c60c6588",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8284a-d5e3-40a0-ada5-2e0932223256",
   "metadata": {},
   "source": [
    "1. Sentence tokenize: Save sentences in file `data/sentences/company_lang_.pkl`\n",
    "2. Embeddings creation: Read the file created in step 1, and create embeddings. Saved in file `data/embeddings/company_lang.pkl`\n",
    "3. Get similarity score for `en2es` and `es2en`. This produces two files, `comparison_company_lang2lang.xlsx`, where `lang2lang`indicates the language of the query (first language) and the best match (second language). **NOTE**: We can choose between two transformers; however, for this task, the `LaBSE` transfomer seems to be the best.\n",
    "4. Get topics using low similarity PR. For each language, we identify the PR with a similarity score in the bottom 20th percentile, in each language. For this subset of PR, we extract the most representative topics, up to a maximum of 10 topics. **NOTE**: For this task, we use the model `paraphrase-multilingual-MiniLM-L12-v2`, since it seems to provide better performance for the topic modeling task.\n",
    "5. Extra Tasks:\n",
    ">- Topic modeling for each company. We store the top 10 topics in an excel file.\n",
    ">- Topics match for each company. We try to match each english topic with the best match (closest cosine similarity score) among the spanish topics.\n",
    "Note that these extra tasks do not seem to be too informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cffa84-2dfd-446e-b785-56ed7c21b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0/1]\t** Sentences and Embeddings Creation for Company 'Endesa' **\n",
      "\n",
      "[S1.] Tonekize sentences\n",
      "\t - Language : english\n",
      "\t[S1-en.] \t Sentences tonenized saved to disk. Total nr. sentences = 21158.\n",
      "\t - Language : spanish\n",
      "\t[S1-es.] \t Sentences tonenized saved to disk. Total nr. sentences = 28560.\n",
      "[S1.] Done with tonekize sentences\n",
      "\n",
      "[S2.] Embeddings creation\n",
      "[S2-en.] \t Starting with embeddings creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db59687083b34e3a9878ad0e72dfb355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S2-en.] \t Embeddings file data/embeddings/Endesa_en.pkl saved to disk.\n",
      "[S2-es.] \t Starting with embeddings creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153e7eaffb0a4761bae419d6c567fef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S2-es.] \t Embeddings file data/embeddings/Endesa_es.pkl saved to disk.\n",
      "[S2.] Done with embeddings creation\n",
      "\t [NOTE] Similarity scores computed using 'laBSE' transformer\n",
      "** Sentence Transformers (match) for Company 'Endesa' **\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822ddc15d4a546579750dd01099f4d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endesa         \t similarity score from 'en' to 'es' = 0.942.\n",
      "Comparison file saved to disk : 'results/best_matching/comparison_Endesa_en2es.xlsx'.\n",
      "╒════╤═══════════╤══════════════╤═════════╤═════════╕\n",
      "│    │ company   │   similarity │   nr_en │   nr_es │\n",
      "╞════╪═══════════╪══════════════╪═════════╪═════════╡\n",
      "│  0 │ Endesa    │     0.941519 │     714 │     965 │\n",
      "╘════╧═══════════╧══════════════╧═════════╧═════════╛\n",
      "\t [NOTE] Similarity scores computed using 'laBSE' transformer\n",
      "** Sentence Transformers (match) for Company 'Endesa' **\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1bc3add9074fcd9bb4fcbc073e71cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endesa         \t similarity score from 'es' to 'en' = 0.906.\n",
      "Comparison file saved to disk : 'results/best_matching/comparison_Endesa_es2en.xlsx'.\n",
      "╒════╤═══════════╤══════════════╤═════════╤═════════╕\n",
      "│    │ company   │   similarity │   nr_en │   nr_es │\n",
      "╞════╪═══════════╪══════════════╪═════════╪═════════╡\n",
      "│  0 │ Endesa    │     0.905756 │     714 │     965 │\n",
      "╘════╧═══════════╧══════════════╧═════════╧═════════╛\n",
      "** Extracting topics on low similarity PR for Company 'Endesa' **\n",
      "\t - Language : english\n",
      "\tExtracting 143 PR with low similarity...\n",
      "[S1-s.] \t Tonekize sentences\n",
      "[S1-e.] \t Sentences tonenized saved to disk. Total nr. sentences = 4137.\n",
      "[S2-s.] \t Starting with embeddings creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fd1633af2e410a9965cc9c33c9738f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S2-e.] \t Embeddings file data/embeddings/Endesa_lowsim_en.pkl saved to disk.\n",
      "[S2.] \t Embeddings imported from data/embeddings/Endesa_lowsim_en.pkl. Starting topic modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 13:31:32,788 - BERTopic - Reduced dimensionality\n",
      "2024-01-17 13:31:32,861 - BERTopic - Clustered reduced embeddings\n",
      "2024-01-17 13:31:59,098 - BERTopic - Reduced number of topics from 108 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S2.] \t Topics Modeled\n",
      "   Topic  Count                                       Name\n",
      "0     -1   1298     -1_customers_mobility_plan_electricity\n",
      "1      0   1555         0_2023_energy_transition_renewable\n",
      "2      1    380          1_charging_electric_mobility_grid\n",
      "3      2    338            2_million_income_euros_dividend\n",
      "4      3    215      3_euroleague_women_festival_finalists\n",
      "5      4    159  4_biodiversity_species_conservation_birds\n",
      "6      5     72                5_thermal_coal_fired_boiler\n",
      "7      6     59      6_basilica_saint_illumination_vatican\n",
      "8      7     44               7_donated_masks_aid_families\n",
      "9      8     17     8_board_corporate_reputation_directors\n",
      "Topics details for company Endesa saved to disk. File 'results/topics/topics_details_lowsim_Endesa_en.xlsx'\n",
      "\t - Language : spanish\n",
      "\tExtracting 193 PR with low similarity...\n",
      "[S1-s.] \t Tonekize sentences\n",
      "[S1-e.] \t Sentences tonenized saved to disk. Total nr. sentences = 6101.\n",
      "[S2-s.] \t Starting with embeddings creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e240dd5845642ef92a542f61d2a7fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S2-e.] \t Embeddings file data/embeddings/Endesa_lowsim_es.pkl saved to disk.\n",
      "[S2.] \t Embeddings imported from data/embeddings/Endesa_lowsim_es.pkl. Starting topic modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 13:32:37,338 - BERTopic - Reduced dimensionality\n",
      "2024-01-17 13:32:37,452 - BERTopic - Clustered reduced embeddings\n",
      "2024-01-17 13:33:05,019 - BERTopic - Reduced number of topics from 130 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S2.] \t Topics Modeled\n",
      "   Topic  Count                                           Name\n",
      "0     -1   1995          -1_proyecto_proyectos_prensa_director\n",
      "1      0   3442             0_descargar_docx_energética_social\n",
      "2      1    291  1_especies_biodiversidad_conservación_hábitat\n",
      "3      2    128              2_piano_conciertos_madrid_cultura\n",
      "4      3     70          3_solares_solar_paneles_fotovoltaicas\n",
      "5      4     49         4_helicóptero_drones_eléctricas_robots\n",
      "6      5     41           5_miel_apicultores_apicultor_carmona\n",
      "7      6     39      6_baterías_batería_condensadores_turbinas\n",
      "8      7     25                 7_hoteles_silken_hotelera_2022\n",
      "9      8     21                    8_jaguar_rover_suv_juicebox\n",
      "Topics details for company Endesa saved to disk. File 'results/topics/topics_details_lowsim_Endesa_es.xlsx'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4eda0c63bb14c20bd4483095d3cec8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════╤════════════════╤═════════╤═════════╤═══════════════════════╤═══════════════════════╕\n",
      "│    │ company        │   nr_en │   nr_es │   similarity_en_to_es │   similarity_es_to_en │\n",
      "╞════╪════════════════╪═════════╪═════════╪═══════════════════════╪═══════════════════════╡\n",
      "│  1 │ ACS            │     318 │     323 │              0.969719 │              0.966951 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 14 │ Acciona        │     887 │    1074 │              0.930277 │              0.895565 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  0 │ Acerinox       │     229 │     236 │              0.93387  │              0.934255 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 15 │ Arcelormittal  │     447 │     192 │              0.77529  │              0.828281 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  4 │ BBVA           │     238 │     522 │              0.916734 │              0.78737  │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 16 │ Bancosabadell  │     294 │     770 │              0.94125  │              0.803138 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  2 │ Bancosantander │     698 │    1102 │              0.914989 │              0.863949 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  3 │ Bankinter      │     612 │     613 │              0.951773 │              0.951053 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  5 │ Caixa          │     645 │    2935 │              0.939889 │              0.795946 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 17 │ Cellnex        │     253 │     239 │              0.928249 │              0.935922 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  6 │ Colonial       │      33 │      34 │              0.929559 │              0.931215 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  7 │ Enagas         │     352 │     391 │              0.92838  │              0.924157 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  8 │ Endesa         │     714 │     965 │              0.941519 │              0.905756 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│  9 │ Ferrovial      │     365 │     362 │              0.915678 │              0.915986 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 18 │ Fluidra        │     154 │     157 │              0.934247 │              0.93048  │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 10 │ Grifols        │     257 │     253 │              0.918767 │              0.920675 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 11 │ IAG            │     219 │     235 │              0.908883 │              0.915402 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 12 │ Iberdrola      │     947 │     903 │              0.952832 │              0.95341  │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 13 │ Inditex        │     154 │     154 │              0.938037 │              0.938037 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 19 │ Indra          │     921 │    1570 │              0.931471 │              0.864217 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 20 │ Logista        │     248 │     251 │              0.939693 │              0.939008 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 21 │ Melia          │     614 │     918 │              0.931799 │              0.878844 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 22 │ Merlin         │      81 │      85 │              0.946587 │              0.944107 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 23 │ Naturgy        │     279 │     821 │              0.934456 │              0.813228 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 24 │ Red            │     542 │     580 │              0.93829  │              0.925317 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 25 │ Repsol         │     330 │     410 │              0.935823 │              0.897344 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 26 │ Rovi           │     207 │     211 │              0.933189 │              0.912962 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 27 │ Sacyr          │     243 │     244 │              0.940351 │              0.939357 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 28 │ Solaria        │     171 │     172 │              0.916651 │              0.913492 │\n",
      "├────┼────────────────┼─────────┼─────────┼───────────────────────┼───────────────────────┤\n",
      "│ 29 │ Telefonica     │     771 │    1837 │              0.927124 │              0.814919 │\n",
      "╘════╧════════════════╧═════════╧═════════╧═══════════════════════╧═══════════════════════╛\n",
      "CPU times: user 35min 45s, sys: 6min 29s, total: 42min 14s\n",
      "Wall time: 11min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "companies = [\"Endesa\"]\n",
    "labels = [\"en\", \"es\"]\n",
    "languages = [\"english\", \"spanish\"]\n",
    "bert_languages = [\"english\", \"spanish\"]\n",
    "\n",
    "topics_analysis = False  # does not seem to be too informative\n",
    "\n",
    "for count, company in enumerate(companies):\n",
    "    print(f\"[{count:2.0f}/{len(companies)}]\\t** Sentences and Embeddings Creation for Company '{company}' **\")\n",
    "    name = company + \".xlsx\"\n",
    "    filename = os.path.join(\"data\", name)\n",
    "    df = pd.read_excel(filename)\n",
    "    df = df[~df.text.isna()]\n",
    "\n",
    "    sentence_tokenize(company, df)\n",
    "    embeddings_creation(company)\n",
    "    \n",
    "    #  NOTE: For Rovi, Santander, and Solaria, see comment above\n",
    "    get_similarity_scores(\n",
    "      company, transformer_type=\"laBSE\"\n",
    "    )  # models = \"laBSE\", \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "    topics_on_low_similarity_pr(company, with_printing=False)\n",
    "\n",
    "    if topics_analysis:\n",
    "        topics_modeling(company)\n",
    "        match_topics(company)\n",
    "        \n",
    "print_summary_similarity(companies_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
